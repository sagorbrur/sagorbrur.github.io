---
permalink: /knowledge-distillation/
title: "Knowledge Distilation"
classes: wide
excerpt: This page contains different resource for knowledge distillation
---
Knowledge distillation is the process of transfering knowledge from a large model to a smaller model.
Smaller model are necessary for less powerful hardware like mobile, edge devices. There are two parts of knowledge distilation.

- Teacher model(large model)
- Student model(distil model)

![kd](/images/knowledge_distillation.png)

Knowledge Distillation are three types:

- Response based distillation
- Feature based distillation
- Relational based distillation

For more information check out the resources below:

## Papers
- [Distilling the Knowledge in a Neural Network by Hinton et al.](https://arxiv.org/pdf/1503.02531.pdf)
- [Knowledge Distillation: A Survey by Gou et al.](https://arxiv.org/pdf/2006.05525.pdf)
- [KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization by Shah et al.](https://arxiv.org/pdf/2011.14691.pdf)

## Blog
- [A beginer guid to knowledge distillation](https://analyticsindiamag.com/a-beginners-guide-to-knowledge-distillation-in-deep-learning/)
- [Knowledge Distillation by Jose Horas](https://josehoras.github.io/knowledge-distillation/)
- [Knowledge Distillation with pytorch](https://koushik0901.medium.com/knowledge-distillation-with-pytorch-40febcf77440)


## Repositories(codes)
- [Keras Knowledge distillation](https://keras.io/examples/vision/knowledge_distillation/)
- [PyTorch Knowledge distillation-KD-Lib](https://github.com/SforAiDl/KD_Lib)
- [Knowledge Distillation in Pytorch](https://github.com/peterliht/knowledge-distillation-pytorch)
- [Awesome knowledge distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)


