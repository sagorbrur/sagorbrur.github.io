---
permalink: /knowledge-distillation/
title: "Knowledge Distilation"
classes: wide
excerpt: This page contains different resource for knowledge distillation
---
Knowledge distillation is the process of transfering knowledge from a large model to a smaller model.
Smaller model are necessary for less powerful hardware like mobile, edge devices. There are two parts of knowledge distilation.

- Teacher model(large model)
- Student model(distil model)

![kd](/images/knowledge_distillation.png)

Knowledge Distillation are three types:

- Response based distillation
- Feature based distillation
- Relational based distillation

For more information check out the resources below:

## Papers
- [Distilling the Knowledge in a Neural Network by Hinton et al.](https://arxiv.org/pdf/1503.02531.pdf)
- [Knowledge Distillation: A Survey by Gou et al.](https://arxiv.org/pdf/2006.05525.pdf)

## Blog
- [A beginer guid to knowledge distillation](https://analyticsindiamag.com/a-beginners-guide-to-knowledge-distillation-in-deep-learning/)

## Repositories(codes)
- [Keras Knowledge distillation](https://keras.io/examples/vision/knowledge_distillation/)
- [Awesome knowledge distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)


