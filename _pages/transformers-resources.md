---
permalink: /transformers/
title: "Transformers Resources"
classes: wide
excerpt: This page contains different resource for learning about the Transformers
---

## Papers
- [Attention is all you need(Vashani 2017)](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(Devlin 2018)](https://arxiv.org/abs/1810.04805)
- [GPT-2: Language Models are Unsupervised Multitask Learners(Radford 2018)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-2: Language Models are Few-Shot Learners(Radford 2020)](https://arxiv.org/abs/2005.14165)

## Codes
- [attention implementation by sooftware](https://github.com/sooftware/attentions/blob/master/attentions.py)
- [notebook on transformers library by leis et al](https://github.com/nlp-with-transformers/notebooks)
- [minGPT by karpathy](https://github.com/karpathy/minGPT)
- [tranformers library](https://github.com/huggingface/transformers)
- [transformers in pytorch](https://github.com/tunz/transformer-pytorch)
- [gpt2 simple python package](https://github.com/minimaxir/gpt-2-simple)

## Blog
- [annotated transformers](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Illustrated gpt-2 by jalammar](https://jalammar.github.io/illustrated-gpt2/)
- [transformers from scratch by abstrukt](https://blog.abstrukt.co/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html)
- [Transformers Recipe](https://github.com/dair-ai/Transformers-Recipe/blob/main/README.md)